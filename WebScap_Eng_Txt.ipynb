{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y3nPKzkn-wS",
        "outputId": "063566d7-6486-4bfa-e1af-5bfb8f3fee4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSGyvQ8godc3"
      },
      "outputs": [],
      "source": [
        "# Define the path to save the file in Google Drive\n",
        "save_path = \"/content/drive/MyDrive/English_Text.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAriuFcKkx51",
        "outputId": "c79e396f-c507-4be0-a22d-0497ee430da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped data saved to /content/drive/MyDrive/English_Text.txt\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Mount Google Drive in Colab.\n",
        "# Scrape text from the target website.\n",
        "# Save the extracted text to a file in Google Drive.\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# URL of the webpage to scrape (replace with actual URL)\n",
        "url = 'https://www.bbc.com/news'\n",
        "\n",
        "# Send a GET request to the webpage\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Find all paragraph tags\n",
        "paragraphs = soup.find_all('p')\n",
        "\n",
        "# Extract text and save to a file\n",
        "with open(save_path, 'w', encoding='utf-8') as file:\n",
        "    for para in paragraphs:\n",
        "        text = para.get_text().strip()\n",
        "        if text:\n",
        "            file.write(text + \"\\n\\n\")\n",
        "\n",
        "print(f\"Scraped data saved to {save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2u4cCLXq-w4",
        "outputId": "ae0ea50e-a6b2-45d4-e1f0-94adbdfd7a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped data saved to /content/drive/MyDrive/English_Text_1.xlsx\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Define the path to save the file in Google Drive\n",
        "save_path = \"/content/drive/MyDrive/English_Text_1.xlsx\"\n",
        "\n",
        "# URL of the webpage to scrape (replace with actual URL)\n",
        "url = 'https://www.cdc.gov/oral-health/data-research/facts-stats/index.html'\n",
        "# Send a GET request to the webpage\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Find all paragraph tags\n",
        "paragraphs = soup.find_all('p')\n",
        "\n",
        "# Extract text and store in a list\n",
        "data = []\n",
        "for para in paragraphs:\n",
        "    text = para.get_text().strip()\n",
        "    if text:\n",
        "        data.append([text])  # Store each paragraph as a row in a list\n",
        "\n",
        "# Convert list to DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"Text\"])\n",
        "\n",
        "# Save the DataFrame to an Excel file (without encoding parameter)\n",
        "df.to_excel(save_path, index=False, engine='openpyxl')\n",
        "\n",
        "print(f\"Scraped data saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5zeg9pMGS1v",
        "outputId": "f162d729-f978-480e-9343-9e5fca8725a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20250327 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250327 pdfplumber-0.11.6 pypdfium2-4.30.1\n",
            "PDF text extracted and saved to: /content/drive/MyDrive/English_Text_2.xlsx\n"
          ]
        }
      ],
      "source": [
        "# To scrape text from a web-based PDF, you need to:\n",
        "!pip install pdfplumber\n",
        "\n",
        "import requests\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Define paths\n",
        "pdf_url = \"https://agriculture.gov.ng/wp-content/uploads/2024/06/HarvestPlus-Recipe-Book.pdf\"\n",
        "pdf_path = \"/content/harvestplus_recipe_book.pdf\"  # Temporary location\n",
        "save_path = \"/content/drive/MyDrive/English_Text_2.xlsx\"  # Save in Google Drive\n",
        "\n",
        "# Download the PDF\n",
        "response = requests.get(pdf_url)\n",
        "with open(pdf_path, \"wb\") as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Extract text from PDF\n",
        "text_data = []\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    for page in pdf.pages:\n",
        "        text = page.extract_text()\n",
        "        if text:\n",
        "            text_data.append([text.strip()])  # Store each page's text as a row\n",
        "\n",
        "# Convert list to DataFrame\n",
        "df = pd.DataFrame(text_data, columns=[\"Extracted Text\"])\n",
        "\n",
        "# Save extracted text to Excel\n",
        "df.to_excel(save_path, index=False, engine='openpyxl')\n",
        "\n",
        "print(f\"PDF text extracted and saved to: {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_vMCB4PxuDY"
      },
      "source": [
        "### SCRAPING HEALTHLINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJVGTiQ-zVkI",
        "outputId": "bfba7f38-8596-46b2-b033-f1e1fab7d932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH1f90txzWVE"
      },
      "outputs": [],
      "source": [
        "# Define the path to save the file in Google Drive\n",
        "save_path = \"/content/drive/MyDrive/English_Text.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW75cciHxzPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70f5c9f7-07b5-4331-dd75-6ae0f2bc9d22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped data saved to /content/drive/MyDrive/English_Text_1.xlsx\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Define the path to save the file in Google Drive\n",
        "save_path = \"/content/drive/MyDrive/English_Text_1.xlsx\"\n",
        "\n",
        "# URL of the webpage to scrape (replace with actual URL)\n",
        "url = 'https://www.healthline.com/health/disordered-eating-vs-eating-disorder#resources'\n",
        "# Send a GET request to the webpage\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Find all paragraph tags\n",
        "paragraphs = soup.find_all('p')\n",
        "\n",
        "# Extract text and store in a list\n",
        "data = []\n",
        "for para in paragraphs:\n",
        "    text = para.get_text().strip()\n",
        "    if text:\n",
        "        data.append([text])  # Store each paragraph as a row in a list\n",
        "\n",
        "# Convert list to DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"Text\"])\n",
        "\n",
        "# Save the DataFrame to an Excel file (without encoding parameter)\n",
        "df.to_excel(save_path, index=False, engine='openpyxl')\n",
        "\n",
        "print(f\"Scraped data saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trying another method to get more data"
      ],
      "metadata": {
        "id": "Vy50dDRUzNJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Define the path to save the file in Google Drive\n",
        "save_path = \"/content/drive/MyDrive/English_Text_1c.xlsx\"\n",
        "\n",
        "# URL of the webpage to scrape\n",
        "url = 'https://www.healthline.com/health/disordered-eating-vs-eating-disorder#resources'\n",
        "\n",
        "# Send a GET request to the webpage\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}  # Mimic a real browser request\n",
        "response = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract text from different tags\n",
        "data = []\n",
        "for tag in soup.find_all(['p', 'h1', 'h2', 'h3', 'li', 'span', 'div']):\n",
        "    text = tag.get_text().strip()\n",
        "    if text:\n",
        "        data.append([text])  # Store each text block as a row\n",
        "\n",
        "# Convert list to DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"Text\"])\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "df.to_excel(save_path, index=False, engine='openpyxl')\n",
        "\n",
        "print(f\"Scraped data saved to {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVIeWUdfzIaH",
        "outputId": "164ba19a-5889-49e2-d718-59bc82f94c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped data saved to /content/drive/MyDrive/English_Text_1c.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OxP5RFPc34w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CiGPNtBh34d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Define the path to save the file in Google Drive\n",
        "save_path = \"/content/drive/MyDrive/English_Text_1.xlsx\"\n",
        "\n",
        "# URL of the webpage to scrape\n",
        "url = 'https://www.healthline.com/health/disordered-eating-vs-eating-disorder#resources'\n",
        "\n",
        "# Send a GET request to the webpage\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}  # Mimic a real browser request\n",
        "response = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract and structure the text data\n",
        "data = []\n",
        "current_section = \"Introduction\"  # Default section\n",
        "\n",
        "for tag in soup.find_all(['h1', 'h2', 'h3', 'p']):  # Extract only relevant elements\n",
        "    text = tag.get_text().strip()\n",
        "    if text:\n",
        "        if tag.name in ['h1', 'h2', 'h3']:  # If it's a heading, update the section\n",
        "            current_section = text\n",
        "        else:  # If it's a paragraph, store it under the current section\n",
        "            data.append([current_section, text])\n",
        "\n",
        "# Convert list to DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"Section\", \"Text\"])\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "df.to_excel(save_path, index=False, engine='openpyxl')\n",
        "\n",
        "print(f\"Scraped data saved to {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBv0Kf_bw470",
        "outputId": "cbbc1a0f-9f6d-46e0-8cbf-1395aa37ef7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped data saved to /content/drive/MyDrive/English_Text_1.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ziJYFPIG38eF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}